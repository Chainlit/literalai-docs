--- 
title: Literal AI Prompts with LangChain
--- 

With Literal AI you can create, debug and manage [prompts](/concepts/prompt). If you are using [LangChain](/integrations/langchain) in your LLM application, you need to use a different format for prompts. In this guide, you will learn how to convert a Literal AI prompt to a [LangChain Chat Prompt Template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/#chatprompttemplate).

You can combine the prompt with the Langchain integration to log the generations and to track which prompt versions were used to generate them.

## How to convert a Literal AI Prompt to LangChain

First, you pull the prompt from the Literal AI platform. Then, you can format this prompt to LangChain's format. 

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

# pull the prompt from Literal AI
prompt = literal_client.api.get_prompt(name="RAG prompt")

# convert to LangChain prompt
langchain_prompt = prompt.to_langchain_chat_prompt_template()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
    // pull the prompt from Literal AI
    const prompt = await client.api.getPrompt('RAG prompt');

    // convert to LangChain prompt
    const chatPrompt = prompt.toLangchainChatPromptTemplate();
}

main();
```
</CodeGroup>


Here is an example application: 

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

from langchain_openai import ChatOpenAI
from langchain.schema.runnable.config import RunnableConfig
from langchain.schema import StrOutputParser

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

def main():
    # pull the prompt from Literal AI
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    # convert to LangChain prompt
    lc_prompt = prompt.to_langchain_chat_prompt_template()

    model = ChatOpenAI(streaming=True)
    runnable = lc_prompt | model | StrOutputParser()
    
    cb = literal_client.langchain_callback()
    variables = {"foo": "bar"}

    res = runnable.invoke(
        variables, 
        config=RunnableConfig(callbacks=[cb], run_name="Test run")
        )

main()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatOpenAI } from '@langchain/openai';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
  // pull the prompt from Literal AI
  const prompt = await client.api.getPrompt('RAG prompt');
  if (!prompt) return;

  // convert to LangChain prompt
  const chatPrompt = prompt.toLangchainChatPromptTemplate();

  const model = new ChatOpenAI({});
  const outputParser = new StringOutputParser();

  const chain = chatPrompt.pipe(model).pipe(outputParser);

  const variables = { foo: 'bar' };

  const response = await chain.invoke(
    variables,
    {
      runName: 'Test run',
      callbacks: [client.instrumentation.langchain.literalCallback()]
    }
  );
}

main();
```

</CodeGroup>