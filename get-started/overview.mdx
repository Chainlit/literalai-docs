---
title: Overview
description: Literal AI is the collaborative platform for building **production-grade LLM apps**.
icon: rocket
---
Literal AI is the collaborative **observability**, **evaluation** and **analytics** platform for building **production-grade LLM apps**. Literal AI offers multimodal logging, including vision, audio, and video.

<Frame caption="Collaborative Flow on Literal AI">
  <img src="/images/features.svg" alt="Literal AI Features"/>
</Frame>

It covers a wide range of LLM-based use cases such as agentic applications, RAG, chatbots and task automation. Literal AI integrates seemlessly with many third parties such as [OpenAI](/integrations/openai), [LangChain/LangGraph](/integrations/langchain) or [Llama Index](/integrations/llama-index).

<Note>
Literal AI is developed by the builders of [Chainlit](https://github.com/Chainlit/chainlit), the open-source Conversational AI Python framework.
</Note>


## Key features

1. [Logs:](/guides/logs) Instrument your code with the Literal AI SDK to log your LLM app in production.

2. [Prompt Management:](/guides/prompt-management) Safely create, A/B test, debug, and version prompts directly from Literal AI.

3. [Dataset:](/guides/dataset) Create datasets mixing production data and hand written examples to run non regression tests/experiments.

4. [Evaluation:](/guides/scorers) Evaluate and monitor the performance of your LLM app in production. View LLM metrics in a dashboard, set automated rules and collect product & user analytics.

<Frame caption="Literal AI Platform Overview">
  <img src="/images/literal-ai-overview.png" alt="Literal AI Platform"/>
</Frame>


## Next up

<CardGroup cols={2}>

<Card title="Instrument your Code" icon="code" color="#FF0581" href="/get-started/quick-start">
  Install the Literal AI SDK and get your API key.
</Card>

<Card title="Create your First Prompt" icon="sparkles" color="#2eb88a" href="/guides/playground">
  Create, version and A/B test your prompts in the Prompt Playground.
</Card>

</CardGroup>




## Why evaluate?

Evaluation is key to enable continuous deployment of LLM-based applications and guarantee that newer
versions perform better than previous ones. To best capture the **user experience** one must understand the
multiple steps which make up the application. As AI applications grow in complexity, they tend to chain 
multiple steps.

Literal AI lets you log & monitor the various steps of your LLM application. By doing so, you can continuously improve the performance of your LLM system, building the most relevant metrics:

| Level          | Metrics                                       |
|----------------|--------------------------------------------|
| LLM Generation | Hallucination, Toxicity, etc.                 |
| Agent Run      | Task completion, Number of intermediate steps |
| Conversation Thread         | User satisfaction                             |


An example is the vanilla **Retrieval Augmented Generation** (RAG), which augments Large Language Models
(LLMs) with domain-specific data. Examples of metrics you can score against are: context relevancy, faithfulness, answer relevancy, etc.


## How to think about evaluation?

Scores are a crucial part of developing and improving your LLM application or agent.

| Who?  | When?           | Type of eval metrics | Example                                             |
|---------------|------------------|------------------|-----------------------------------------------------|
| End-User      | In Production     | Explicit Feedback (üëçüëé)        | Thumbs-up or down on a chatbot's answer             |
| End-User      | In Production     | Implicit Feedback based on product metric        | User conversion to paid offering increases by 15%   |
| LLM-as-a-Judge | In Production  | AI evaluation (without ground truth) | Hallucination, context relevancy, etc. |
| LLM-as-a-Judge | During Iteration  | AI evaluation against a Dataset (with ground truth or not) | Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc. |
| Domain Expert | During Iteration  | Human evaluation against a Dataset (with ground truth or not)  | Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc. |

