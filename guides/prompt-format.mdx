--- 
title: Prompt Formatting
--- 

Prompts are the input for LLMs, instructions or questions to complete a task or generate a response. Prompt formatting refers to the structure, style and layout used in a prompt. How a prompt is formatted influences the quality and relevance of the generated text. So it is important to format a prompt right. 

## Prompt Template
In Literal AI, you can create Prompt Templates in the playground or in code. A prompt template is a predefined structure that is used to create prompts. A prompt template is a reusable framework, consisting of instructions to the system and variables. Variables are used to insert specific information, like the question of the user, the context and previous responses by the LLM. 

A prompt template in LiteralAI can look as follows in code. It consists of a name, messages, tools and LLM model settings. Template Messages and Tools can contain variables, which are written like `{{variable}}` following the [the Mustache prompt templating format](https://scalate.github.io/scalate/documentation/mustache.html#Syntax). 

<CodeGroup>
```json Prompt Template
{
  "name": "RAG prompt",
  "template_messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant identifying questions related to Literal AI documentation. Keep it short, and if available prefer responding with code."
    }, {
      "role": "user", 
      "content": "Answer the question based on the context below. \n
        Context: {{#context}} {{.}} {{/context}} \n
        Question: {{question}} \n
        Answer:"
    }
  ],
  "settings": {
    "provider": "openai",
    "stop": [],
    "model": "gpt-4-turbo-preview",
    "top_p": 1,
    "max_tokens": 512,
    "temperature": 0,
    "presence_penalty": 0,
    "frequency_penalty": 0
  }
}
```
</CodeGroup>


## Prompt Formatting

To use a Prompt Template in your application, you need to format this template with the relevant variables of the thread. With the Literal AI clients, you can format messages in the OpenAI format. This allows you to use prompts with integrations like OpenAI. Combining prompts with integrations (like the [OpenAI integration](/integrations/openai)) allows you to log the generations and to track which prompt versions were used to generate them.

Prompt formatting with the clients: 

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

# pull the prompt from Literal AI
prompt = literal_client.api.get_prompt(name="RAG prompt")

# format the prompt with a variable
query = "What are the features of Literal AI?"
variables = {"question": query}
messages = prompt.format_messages(variables)
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
    // pull the prompt from Literal AI
    const prompt = await client.api.getPrompt('RAG prompt');

    // format the prompt with a variable
    const variables = { query: 'What are the features of Literal AI?' };
    const messages = prompt.formatMessages(variables);
}

main();
```
</CodeGroup>

In an application using OpenAI, it can look as follows: 

<CodeGroup>
```python Python
import os
import asyncio
from literalai import LiteralClient
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
# Optionally instrument the openai client to log generations
literal_client.instrument_openai()

async def main():
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    # Optionally pass variables to the prompt
    query = "What are the features of Literal AI?"
    variables = {"question": query}
    messages = prompt.format_messages(variables)

    # Optionally pass settings
    settings = {"temperature": 0, "stream": True, "model": "gpt-4-turbo-preview"}

    stream = await openai_client.chat.completions.create(
        messages=messages,
        # Optionally pass the tools defined in the prompt
        tools=prompt.tools,
        # Pass the settings defined in the prompt
        **settings,
        stream=True
    )

    async for chunk in stream:
        print(chunk)

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';
import OpenAI from 'openai';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

const openai = new OpenAI({
  apiKey: process.env['OPENAI_API_KEY'],
});

async function main() {
  const prompt = await client.api.getPrompt('RAG prompt');
  if (!prompt) return;

  // Optionally pass variables to the prompt
  const variables = { query: 'What are the features of Literal AI?' };

  const messages = prompt.formatMessages(variables);

  const result = await openai.chat.completions.create({
    ...prompt.settings,
    tools: prompt.tools,
    messages: messages,
    stream: true
  });

  await client.instrumentation.openai(result);
}

main();

```
</CodeGroup>