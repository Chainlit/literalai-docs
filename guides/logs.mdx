---
title: Logs
description: Logs are essential to monitor and improve your LLM app in production. Literal AI provides a flexible and composable SDK to log your LLM app at different levels of granularity.
icon: list-tree
---

## Semantics

<Frame caption="Log Hierarchy on Literal AI">
  <img src="/images/thread-steps.svg" alt="Literal AI Platform"/>
</Frame>

Literal AI approaches LLM logging at three levels:
1. **Generation**: Log of a single LLM call. (Generations are Steps.)
2. **Run**: Trace of an Agent/Chain run, including its intermediate steps. Can contain one or multiple generations. 
3. **Thread**: A collection of Runs that are part of a single conversation.

<Frame caption="A Thread containing a run and intermediate steps in Literal AI">
  <img src="/images/threads-overview.gif" alt="Literal AI Platform"/>
</Frame>

You can log a generation only (typically for extraction use cases), or log a run only (typically for task automation), or combine them in threads (typically for chatbots). 

<Note>See [installation](/get-started/quick-start) to get your API key and instantiate the SDK</Note>

## Log an LLM Generation
Generations are logged by [integrations](/integrations) with LLM providers. They capture the prompt, completion, settings, and token latency.

Here is an example with OpenAI:

<CodeGroup>
```python Python
literalai_client.instrument_openai()

# Run a regular OpenAI chat completion
from openai import OpenAI

oai = OpenAI()

def call_openai(user_input: str):
    oai.chat.completions.create(
        model="gpt-4o",
        messages=[{ "role": "user", "content": user_input }]
    )

call_openai("Hello world")
```

```typescript TypeScript
literalAiClient.instrumentation.openai();

const openai = new OpenAI();

async function callOpenAI(userInput: string) {
  const stream = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages: [{ role: 'user', content: userInput }]
  });
};
callOpenAI("Hello world");
```
</CodeGroup>

### Multimodal LLM
You can leverage multimodal capabilities on Literal AI in two ways:
- Simple logging on API calls to Multimodal LLM APIs, like `gpt-4o`
- Save multimodal files as [Attachments](/guides/logs#add-an-attachment). Image, videos, audio and other files are shown as `Attachment` in the Literal AI platform, which can be accessed and downloaded via a `Step`. 

<Frame caption="Example of a logged multimodal LLM call">
  <img
    src="/images/multimodal-playground.jpeg"
    alt="A logged multimodal LLM call"
  />
</Frame>


## Log a Run

A **Run** represents a trace of an Agent or Chain execution, capturing all intermediate steps and actions.

Runs can be logged manually using decorators or through framework integrations such as [Llama Index](/integrations/llama-index) or [LangChain](/integrations/langchain).

### Logging a Run with Intermediate Steps

Hereâ€™s how you can log a Run with intermediate steps using Python and TypeScript:

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def get_temperature(city: str):
    return "10C"

@literalai_client.run
def my_agent(user_input: str):
    # Reusing the OpenAI call from the previous example
    call_openai(user_input)
    # Naive tool example
    get_temperature("paris")
    return "Success"
    
my_agent("Hello world")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
async function getTemperature(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      name: 'Get Temperature',
      input: { city },
    })
    .wrap(() => {
      return { content: '10C' };
    });
}

async function myAgent(userInput: string) {
  return literalAiClient
    .run({
      name: 'My Agent',
      input: { userInput }
    })
    .wrap(async () => {
      // Reusing the OpenAI call from the previous example
      await callOpenAI(userInput);
      // Naive tool example
      await getTemperature("Paris");
      return { content: 'Success' };
    });
}

myAgent("Hello world");
```
</CodeGroup>

### Adding Metadata and Tags to Steps

**Tags** and **Metadata** can be added to both Runs and Steps to provide additional context and facilitate filtering and categorization.

<CodeGroup>
```python Python
@literalai_client.run
def my_step(input):
    current_step = literalai_client.get_current_step()
    # some code, llm call, tool call, etc.
    current_step.metadata = {"region": "europe"}
    current_step.tags = ["to_review"]
    return "answer"
```

```typescript TypeScript
import { LiteralClient } from "@literalai/client";

const literalAiClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

// The Assistant could have intermediary steps
async function myRun() {
  return literalAiClient
    .step({
      type: 'run',
      name: 'My Run',
      input: { content: 'My run Input' },
      metadata: { region: "europe" },
      tags: ["to_review"],
    })
    .wrap(() => {
      return { content: 'Success' };
    });
}
```
</CodeGroup>

### Adding Attachments to Steps

You can attach files to a Run or any of its intermediate steps, which is particularly useful for multimodal use cases.

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def load_document():
    with open ("./some.pdf", "rb") as file:
        literalai_client.create_attachment(
          name="pdf_document",
          content=file.read()
          )
    return "doc loaded"
```

```typescript TypeScript
async function loadDocument(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      input: { document },
    })
    .wrap(() => {
      const fileStream = fs.createReadStream('./some.pdf');
      const mime = 'application/pdf';
      await literalAiClient.api.createAttachment({
        content: fileStream,
        mime,
        name: 'pdf_document'
      });
      return { content: "doc loaded" };
    });
}
```
</CodeGroup>

### Learn More

The intermediate steps and the agent itself are logged using the `Step` class. You can learn more about the Step API in the following references:

<CardGroup cols={2}>
<Card title="Python Step API reference" icon="python" color="#2eb88a" href="/python-client/api-reference/step">
    Learn how to use the Python Step API.
</Card>

<Card title="TypeScript Step API reference"  color="#FF0581" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Step API.
</Card>
</CardGroup>


## Log a Thread

You can interact with an example Thread in the platform [here](https://cloud.getliteral.ai/thread/b3b61ec8-0d8a-444d-9e70-d6929c1129d1). 

It is up to the application to keep track of the thread ID and pass it to the Literal AI client.
Every run logged with the same thread ID will be part of the same conversation.

Here is an example:

<CodeGroup>
```python Python
import uuid

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)

thread_id = str(uuid.uuid4())
# Calling the agent a first time
process_message(thread_id=thread_id, user_input="foo")
# Calling the agent a second time with the same thread ID
process_message(thread_id=thread_id, user_input="bar")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
import { v4 as uuidv4 } from 'uuid';

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}

const threadId = uuidv4();
// Calling the agent a first time
processMessage(threadId, "foo");
// Calling the agent a second time with the same thread ID
processMessage(threadId, "bar");
```
</CodeGroup>

You can learn more about the Thread API in the following references:

<CardGroup cols={2}>

<Card title="Python Thread API reference" icon="python" color="#2eb88a" href="/python-client/api-reference/thread">
    Learn how to use the Python Thread API.
</Card>

<Card title="TypeScript Thread API reference"  color="#FF0581" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Thread API.
</Card>

</CardGroup>

### Bind a Thread to a User

You can bind a thread to a user to track the user's activity. This is useful for chatbots or any other conversational AI.

To do so, you need to use a common identifier for the user, such as an email or a user ID:

<CodeGroup>
```python Python
user = literalai_client.api.get_or_create_user(identifier="John Doe")

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(participant_id=user.id, thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)
```
```typescript TypeScript
const participantId = await literalAiClient.api.getOrCreateUser('John Doe');

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ participantId, id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}
```
</CodeGroup>


## Log a Distributed Trace

<Card title="Distributed Tracing Cookbook" icon="github" href="https://github.com/Chainlit/literalai-cookbooks/tree/main/python/distributed-tracing">
    Learn how to log distributed traces with Literal AI.
</Card>


## Add a Score

<Note>**Scores** allow you to evaluate the LLM system performance at three levels: LLM generations, Agent Runs and Conversation Threads.</Note>

Scores can be human generated (human feedback, like a thump up or down), or AI generated (hallucination evaluation for instance).

They can be visualized on the dashboard charts and used as filters.

### Add a User Feedback
<CodeGroup>
```python Python
def add_user_feedback(run_id: str, value: int, comment: str):
    literalai_client.api.create_score(
        step_id=run_id,
        name="user-feedback",
        type="HUMAN",
        value=value,
        comment=comment,
    )
```

```typescript TypeScript
async function addUserFeedback(runId: string, value: number, comment: string) {
    const score = await literalClient.api.createScore({
        stepId: runId,
        name: 'user-feedback',
        type: 'HUMAN',
        comment,
        value,
      });
};
```
</CodeGroup>


### Add a Product-Related Metric
Correlate your LLM system to a product metric, such as conversion, churn, upsell, etc.
This can be done by:
- Adding a specific product-related score on Literal AI.
- Sending the logged run id to your analytics system, such as PostHog or Amplitude. 

### Add an AI Evaluation Result
Refer to [Evaluation](/guides/evaluation.mdx)


## Fetch Existing Logs

You can fetch existing logs using the SDKs. Here is an example to fetch the last 5 threads where a user participated:

<CodeGroup>
```python Python
from literalai import LiteralClient

literalai_client = LiteralClient()

user_id = 'uuid'

threads = literalai_client.api.list_threads(
    first=5,
    filters=[{"operator": "eq", "field": "participantId", "value": user_id}]
)

for d in threads.data:
    print(d.to_dict())

literalai_client.flush_and_stop()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';
const literalAiClient = new LiteralClient("YOUR_API_KEY");

async function main() {

  const userId = 'uuid'
  const threads = await literalAiClient.api.getThreads({first: 5, filters: [{"operator": "eq", "field": "participantId", "value": userId}]});
  console.log(threads);
}
```
</CodeGroup>

More generally, you can fetch any Literal AI object. Check out the SDKs and API reference to learn how.


## On Literal AI
### Filter logs

Leverage the powerful filters on Literal AI. Use these same filters to export your data using the SDKs.
<Frame caption="Filter on logs">
  <img
    src="/images/filter-logs.gif"
    alt="Filter on existing logs"
  />
</Frame>

### Debug logged LLM generations

<Frame caption="Replay a logged LLM generation in the Playground">
  <video
    autoPlay
    muted
    loop
    playsInline
    src="/images/debug-generation.mp4"
  />
</Frame>

### Add Tags and Scores from the UI

You can add tags and scores directly from the user interface.
<Frame caption="Add a Tag to a Thread">
  <img src="/images/add-tag-thread.png" alt="Add a Tag" />
</Frame>

## Conclusion

Logging with Literal AI is composable and unopinionated. It can be done at different levels depending on your use case. 
