---
title: Logs
description: Logs are essential to monitor and improve your LLM app in production. Literal AI provides a flexible and composable SDK to log your LLM app at different levels of granularity.
icon: list-tree
---

## Semantics

<Frame caption="A Thread with Steps and Tags.">
  <img src="/images/thread-steps.svg" alt="A Thread with Steps and Tags." />
</Frame>

Literal AI approaches LLM logging at three levels:
1. **Generation**: Log of a single LLM call. (Generations are Steps.)
2. **Step**: Log of a regular function execution, which is usually an intermediate step in an LLM system. Possible `type`s are: `run`, `tool`, `llm`, `embedding`, `retrieval`, `rerank`, `undefined`, `user_message`, `assistant_message`, `system_message`. Steps can be considered as Spans.
3. **Run**: Trace of an Agent/Chain run, including its intermediate steps. Can contain one or multiple generations. 
4. **Thread**: A collection of Runs that are part of a single conversation.

<Frame caption="A Thread containing a run and intermediate steps in Literal AI">
  <img src="/images/threads-overview.gif" alt="Literal AI Platform"/>
</Frame>

You can log a generation only (typically for extraction use cases), or log a run only (typically for task automation), or combine them in threads (typically for chatbots). 

<Note>See [installation](/get-started/installation) to get your API key and instantiate the SDK</Note>

## Log an LLM Generation
Generations are logged by [integrations](/integrations) with LLM providers. They capture the prompt, completion, settings, and token latency.

Here is an example with OpenAI:

<CodeGroup>
```python Python
literalai_client.instrument_openai()

# Run a regular OpenAI chat completion
from openai import OpenAI

oai = OpenAI()

def call_openai(user_input: str):
    oai.chat.completions.create(
        model="gpt-4o",
        messages=[{ "role": "user", "content": user_input }]
    )

call_openai("Hello world")
```

```typescript TypeScript
literalAiClient.instrumentation.openai();

const openai = new OpenAI();

async function callOpenAI(userInput: string) {
  const stream = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages: [{ role: 'user', content: userInput }]
  });
};
callOpenAI("Hello world");
```
</CodeGroup>

### Multimodal logs
You can leverage multimodal capabilities on Literal AI in two ways:
- Simple logging on API calls to Multimodal LLM APIs, like `gpt-vision`
- Save multimodal files as [Attachments](/guides/logs#add-an-attachment). Image, videos, audio and other files are shown as `Attachment` in the Literal AI platform, which can be accessed and downloaded via a `Step`. 

<Frame caption="Example of a logged multimodal LLM call">
  <img
    src="/images/multimodal-playground.jpeg"
    alt="A logged multimodal LLM call"
  />
</Frame>


## Log a Run

A Run can be logged **manually with decorators** or with **framework integrations** such as [Llama Index](/integrations/llama-index) or [LangChain](/integrations/langchain).

### Log an intermediate Step
<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def get_temperature(city: str):
    return "10C"
# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
async function getTemperature(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      name: 'Get Temperature',
      input: { city },
    })
    .wrap(() => {
      return { content: '10C' };
    });
}
```
</CodeGroup>

#### Add metadata and tags
**Tags** can be assigned to Threads, Steps, and Generations to label and filter data across the application. Examples include tags for data management like "to review" or "reviewed," or for categorizing questions such as "billing" or "return policy."

**Metadata** provides additional context, details, or configuration to customize and enhance the functionality of Threads, Steps, Users, and Generations. It adds unique parameters specific to each object.

**How to add metadat and tags to a Run or a Step**

<CodeGroup>

```python Python
@literalai_client.step(type="run", name="my_step")
def my_step(input):
    current_step = literalai_client.get_current_step()
    # some code, llm call, tool call, etc.
    current_step.metadata = {"region": "europe"}
    current_step.tags = ["to_review"]
    return "answer"

```
```typescript TypeScript
import { LiteralClient } from "@literalai/client";

const literalAiClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

// The Assistant could have intermediary steps
async function myRun() {
  return literalAiClient
    .step({
      type: 'run',
      name: 'My Run',
      input: { content: 'My run Input' },
      metadata: { region: "europe" },
      tags: ["to_review"],
    })
    .wrap(() => {
      return { content: 'Success' };
    });
}
```

</CodeGroup>
#### Add an Attachment

You can attach files to a Run or any of its intermediate steps. It is useful for multimodal use cases.
<Frame caption="Example of attachments">
  <img
    src="/images/attachments.png"
    alt="Attachments on a step"
  />
</Frame>

Here is a sample code to add an attachment.

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def load_document():
    with open ("./some.pdf", "rb") as file:
        literalai_client.create_attachment(
          name="pdf_document",
          content=file.read()
          )
    return "doc loaded"
```

```typescript TypeScript
async function loadDocument(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      input: { document },
    })
    .wrap(() => {
      const fileStream = fs.createReadStream('./some.pdf');
      const mime = 'application/pdf';
      await literalAiClient.api.createAttachment({
        content: fileStream,
        mime,
        name: 'pdf_document'
      });
      return { content: "doc loaded" };
    });
}
```
</CodeGroup>

### Log a Run

You can log traces of agent runs or workflow runs in the following way:

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def get_temperature(city: str):
    return "10C"

@literalai_client.step(type="run")
def my_agent(user_input: str):
    # Reusing the OpenAI call from the previous example
    call_openai(user_input)
    # Naive tool example
    get_temperature("paris")
    return "Success"
    
my_agent("Hello world")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
async function getTemperature(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      name: 'Get Temperature',
      input: { city },
    })
    .wrap(() => {
      return { content: '10C' };
    });
}


async function myAgent(userInput: string) {
  return literalAiClient
    .run({
      name: 'My Agent',
      input: { userInput }
    })
    .wrap(async () => {
      // Reusing the OpenAI call from the previous example
      await callOpenAI(userInput);
      // Naive tool example
      await getTemperature("Paris");
      return { content: 'Success' };
    });
}

myAgent("Hello world");
```
</CodeGroup>

The intermediate steps and the agent itself are logged using the `Step` class. You can learn more about the Step API in the following references:

<CardGroup cols={2}>
<Card title="Python Step API reference" icon="python" color="#2eb88a" href="/python-client/abstractions/step">
    Learn how to use the Python Step API.
</Card>

<Card title="TypeScript Step API reference"  color="#2662d9" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Step API.
</Card>
</CardGroup>


## Log a Thread

You can interact with an example Thread in the platform [here](https://cloud.getliteral.ai/thread/b3b61ec8-0d8a-444d-9e70-d6929c1129d1). 

It is up to the application to keep track of the thread ID and pass it to the Literal AI client.
Every run logged with the same thread ID will be part of the same conversation.

Here is an example:

<CodeGroup>
```python Python
import uuid

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)

thread_id = str(uuid.uuid4())
# Calling the agent a first time
process_message(thread_id=thread_id, user_input="foo")
# Calling the agent a second time with the same thread ID
process_message(thread_id=thread_id, user_input="bar")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
import { v4 as uuidv4 } from 'uuid';

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}

const threadId = uuidv4();
// Calling the agent a first time
processMessage(threadId, "foo");
// Calling the agent a second time with the same thread ID
processMessage(threadId, "bar");
```
</CodeGroup>

You can learn more about the Thread API in the following references:

<CardGroup cols={2}>

<Card title="Python Thread API reference" icon="python" color="#2eb88a" href="/python-client/abstractions/thread">
    Learn how to use the Python Thread API.
</Card>

<Card title="TypeScript Thread API reference"  color="#2662d9" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Thread API.
</Card>

</CardGroup>

### Bind a Thread to a User

You can bind a thread to a user to track the user's activity. This is useful for chatbots or any other conversational AI.

To do so, you need to use a common identifier for the user, such as an email or a user ID:

<CodeGroup>
```python Python
user = literalai_client.api.get_or_create_user(identifier="John Doe")

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(participant_id=user.id, thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)
```
```typescript TypeScript
const participantId = await literalAiClient.api.getOrCreateUser('John Doe');

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ participantId, id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}
```
</CodeGroup>


## Log a distributed trace

Refer to cookbook X

## Add a score
**Scores** allow you to evaluate the LLM system performance at three levels: LLM generations, Agent Runs and Conversation Threads. This can be used to track the performance and accuracy of your system. Scores can be human generated (human feedback, like a thump up or down), or AI generated (hallucination evaluation for instance). A score or feedback can be assigned to a Generation, Step or Thread. Scores can be visualized on the dashboard in charts, and data can be filtered by scores.

### Add a user feedback
<CodeGroup>
```python Python
def add_user_feedback(run_id: str, value: int, comment: str):
    literalai_client.api.create_score(
        step_id=run_id,
        name="user-feedback",
        type="HUMAN",
        value=value,
        comment=comment,
    )
```

```typescript TypeScript
async function addUserFeedback(runId: string, value: number, comment: string) {
    const score = await literalClient.api.createScore({
        stepId: runId,
        name: 'user-feedback',
        type: 'HUMAN',
        comment,
        value,
      });
};
```
</CodeGroup>


### Add a product-related metric
Correlate your LLM system to a product metric, such as conversion, churn, upsell, etc.
This can be done by:
- adding a specific product-related score on Literal AI.
- sending the logged run id to your analytics system, such as PostHog or Amplitude. 

### Add an AI evaluation result
Refer to [Evaluation](/guides/evaluation.mdx)


## Fetch existing logs
You can get all the threads from Literal AI to your program:
<CodeGroup>
```python Python
from literalai import LiteralClient

literalai_client = LiteralClient()

user_id = 'uuid'

threads = literalai_client.api.list_threads(
    first=5,
    filters=[{"operator": "eq", "field": "participantId", "value": user_id}]
)

for d in threads.data:
    print(d.to_dict())

literalai_client.flush_and_stop()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';
const literalAiClient = new LiteralClient("YOUR_API_KEY");

async function main() {

  const userId = 'uuid'
  const threads = await literalAiClient.api.getThreads({first: 5, filters: [{"operator": "eq", "field": "participantId", "value": userId}]});
  console.log(threads);
}
```
</CodeGroup>

More generally, you can fetch any Literal AI object. Check out the SDKs and API reference to learn how.


## On Literal AI
### Filter logs

Leverage the powerful filters on Literal AI. Use these same filters to export your data using the SDKs.
<Frame caption="Filter on logs">
  <img
    src="/images/filter-logs.gif"
    alt="Filter on existing logs"
  />
</Frame>

### Debug logged LLM generations

// TODO

### Add metadata and scores from the UI

You can add metadata tags and scores directly from the user interface.
<Frame caption="Add a Tag to a Thread">
  <img src="/images/add-tag-thread.png" alt="Add a Tag" />
</Frame>

## Conclusion

Logging with Literal AI is composable and unopinionated. It can be done at different levels depending on your use case. 
