---
title: Logs
description: Logs are essential to monitor and improve your LLM app in production. Literal AI provides a flexible and composable SDK to log your LLM app at different levels of granularity.
icon: list-tree
---

## Semantics

TODO: update image
<Frame caption="A Thread with Steps and Tags.">
  <img src="/images/thread-steps.svg" alt="A Thread with Steps and Tags." />
</Frame>

Literal AI approaches LLM logging at three levels:
1. **Generation**: Log of a single LLM call. (Generations are Steps.)
2. **Step**: Log of a regular function execution, which is usually an intermediate step in an LLM system. Possible `type`s are: `run`, `tool`, `llm`, `embedding`, `retrieval`, `rerank`, `undefined`, `user_message`, `assistant_message`, `system_message`. Steps can be considered as Spans.
3. **Run**: Trace of an Agent/Chain run, including its intermediate steps. Can contain one or multiple generations. 
4. **Thread**: A collection of Runs that are part of a single conversation.

<Frame caption="A Thread containing a run and intermediate steps in Literal AI">
  <img src="/images/threads-overview.gif" alt="Literal AI Platform"/>
</Frame>

You can log a generation only (typically for extraction use cases), or log a run only (typically for task automation), or combine them in threads (typically for chatbots). 

<Note>See [installation](/get-started/installation) to get your API key and instantiate the SDK</Note>

## Log an LLM Generation
Generations are logged by [integrations](/integrations) with LLM providers. They capture the prompt, completion, settings, and token latency.

<Frame caption="An example of a Generation on Literal AI">     
  <img
    src="/images/generation-example.png"
    alt="An example of a Generation"
  />
</Frame>

Here is an example with OpenAI:

<CodeGroup>
```python Python
literalai_client.instrument_openai()

# Run a regular OpenAI chat completion
from openai import OpenAI

oai = OpenAI()

def call_openai(user_input: str):
    oai.chat.completions.create(
        model="gpt-4o",
        messages=[{ "role": "user", "content": user_input }]
    )

call_openai("Hello world")
```

```typescript TypeScript
literalAiClient.instrumentation.openai();

const openai = new OpenAI();

async function callOpenAI(userInput: string) {
  const stream = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages: [{ role: 'user', content: userInput }]
  });
};
callOpenAI("Hello world");
```
</CodeGroup>

### Multimodal logs
You can leverage multimodal capabilities on Literal AI in two ways:
- Simple logging on API calls to Multimodal LLM APIs, like `gpt-vision`
- Save multimodal files as [Attachments](/guides/logs#add-an-attachment). Image, videos, audio and other files are shown as `Attachment` in the Literal AI platform, which can be accessed and downloaded via a `Step`. 

<Frame caption="Example of a logged multimodal LLM call">
  <img
    src="/images/multimodal-playground.jpeg"
    alt="A logged multimodal LLM call"
  />
</Frame>


## Log an individual Step
<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def get_temperature(city: str):
    return "10C"
# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
async function getTemperature(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      name: 'Get Temperature',
      input: { city },
    })
    .wrap(() => {
      return { content: '10C' };
    });
}
```
</CodeGroup>

### Add metadata and tags
Metadata serves as a powerful tool for enriching objects with additional context, details, or configuration that can be instrumental in customizing and enhancing the functionality of each object. Specifically, the metadata field is available on Thread, Step, User, and Generation objects within the platform.

**How to add metadata to a Run or a Step**

<CodeGroup>

```python Python
@literalai_client.step(type="run", name="my_step")
def my_step(input):
    current_step = literalai_client.get_current_step()
    # some code, llm call, tool call, etc.
    current_step.metadata = {"region": "europe"}
    return "answer"

```
```typescript TypeScript
import { LiteralClient } from "@literalai/client";

const literalAiClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

// The Assistant could have intermediary steps
async function myRun() {
  return literalAiClient
    .step({
      type: 'run',
      name: 'My Run',
      input: { content: 'My run Input' },
      metadata: { region: "europe" },
    })
    .wrap(() => {
      return { content: 'Success' };
    });
}
```

</CodeGroup>
### Add an Attachment

You can attach files to a Run or any of its intermediate steps. It is useful for multimodal use cases.
<Frame caption="Example of attachments">
  <img
    src="/images/attachments.png"
    alt="Attachments on a step"
  />
</Frame>

Let's add an image to our step tool:

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def load_document():
    with open ("./some.pdf", "rb") as file:
        literalai_client.create_attachment(
          name="pdf_document",
          content=file.read()
          )
    return "doc loaded"
```

```typescript TypeScript
async function loadDocument(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      input: { document },
    })
    .wrap(() => {
      const fileStream = fs.createReadStream('./some.pdf');
      const mime = 'application/pdf';
      await literalAiClient.api.createAttachment({
        content: fileStream,
        mime,
        name: 'pdf_document'
      });
      return { content: "doc loaded" };
    });
}
```
</CodeGroup>



## Log a Run

A Run can be logged manually with decorators or with framework integrations such as [Llama Index](/integrations/llama-index) or [LangChain](/integrations/langchain).

Here is an example with decorators:

<CodeGroup>
```python Python
@literalai_client.step(type="tool")
def get_temperature(city: str):
    return "10C"

@literalai_client.step(type="run")
def my_agent(user_input: str):
    # Reusing the OpenAI call from the previous example
    call_openai(user_input)
    # Naive tool example
    get_temperature("paris")
    return "Success"
    
my_agent("Hello world")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
async function getTemperature(city: string) {
  return literalAiClient
    .step({
      type: 'tool',
      name: 'Get Temperature',
      input: { city },
    })
    .wrap(() => {
      return { content: '10C' };
    });
}


async function myAgent(userInput: string) {
  return literalAiClient
    .run({
      name: 'My Agent',
      input: { userInput }
    })
    .wrap(async () => {
      // Reusing the OpenAI call from the previous example
      await callOpenAI(userInput);
      // Naive tool example
      await getTemperature("Paris");
      return { content: 'Success' };
    });
}

myAgent("Hello world");
```
</CodeGroup>

The intermediate steps and the agent itself are logged using the `Step` class. You can learn more about the Step API in the following references:

<CardGroup cols={2}>
<Card title="Python Step API reference" icon="python" color="#2eb88a" href="/python-client/abstractions/step">
    Learn how to use the Python Step API.
</Card>

<Card title="TypeScript Step API reference"  color="#2662d9" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Step API.
</Card>
</CardGroup>

### Score a Run

Scoring a run is useful to capture product metrics such as:
- Direct User feedback
- Implicit user feedback (user accepted suggestion or not)

Here is an example:

<CodeGroup>
```python Python
def add_user_feedback(run_id: str, value: int, comment: str):
    literalai_client.api.create_score(
        step_id=run_id,
        name="user-feedback",
        type="HUMAN",
        value=value,
        comment=comment,
    )
```

```typescript TypeScript
async function addUserFeedback(runId: string, value: number, comment: string) {
    const score = await literalClient.api.createScore({
        stepId: runId,
        name: 'user-feedback',
        type: 'HUMAN',
        comment,
        value,
      });
};
```
</CodeGroup>


## Log a Thread

You can interact with an example Thread in the platform [here](https://cloud.getliteral.ai/thread/b3b61ec8-0d8a-444d-9e70-d6929c1129d1). 

It is up to the application to keep track of the thread ID and pass it to the Literal AI client.
Every run logged with the same thread ID will be part of the same conversation.

Here is an example:

<CodeGroup>
```python Python
import uuid

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)

thread_id = str(uuid.uuid4())
# Calling the agent a first time
process_message(thread_id=thread_id, user_input="foo")
# Calling the agent a second time with the same thread ID
process_message(thread_id=thread_id, user_input="bar")

# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()
```

```typescript TypeScript
import { v4 as uuidv4 } from 'uuid';

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}

const threadId = uuidv4();
// Calling the agent a first time
processMessage(threadId, "foo");
// Calling the agent a second time with the same thread ID
processMessage(threadId, "bar");
```
</CodeGroup>

You can learn more about the Thread API in the following references:

<CardGroup cols={2}>

<Card title="Python Thread API reference" icon="python" color="#2eb88a" href="/python-client/abstractions/thread">
    Learn how to use the Python Thread API.
</Card>

<Card title="TypeScript Thread API reference"  color="#2662d9" icon="js" href="/typescript-client/with-wrappers">
    Learn how to use the TypeScript Thread API.
</Card>

</CardGroup>

### Bind a Thread to a User

You can bind a thread to a user to track the user's activity. This is useful for chatbots or any other conversational AI.

To do so, you need to use a common identifier for the user, such as an email or a user ID:

<CodeGroup>
```python Python
user = literalai_client.api.get_or_create_user(identifier="John Doe")

def process_message(thread_id: str, user_input: str):
    with literalai_client.thread(participant_id=user.id, thread_id=thread_id) as thread:
        # Reusing the Agent from the previous example
        my_agent(user_input)
```
```typescript TypeScript
const participantId = await literalAiClient.api.getOrCreateUser('John Doe');

async function processMessage(threadId: string, userInput: string) {
    await literalAiClient
        .thread({ participantId, id: threadId})
        .wrap(async () => {
            await myAgent(userInput);
        });
}
```
</CodeGroup>


## Log a distributed trace

Refer to cookbook X

## Add a score
TODO: Score table
### Add a user feedback

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

score = literal_client.api.create_score(
            step_id="<STEP_UUID>",
            name="user-feedback",
            type="HUMAN",
            comment="Hello world",
            value=1,
        )
```

```typescript TypeScript
import { LiteralClient, ChatGeneration} from "@literalai/client";

const literalClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

async function main() {
    const score = await literalClient.api.createScore({
        stepId: '05b217b4-5890-4434-b1a2-1749d842b2db',
        name: 'user-feedback',
        type: 'HUMAN',
        comment: 'Example score',
        value: 1,
      });
      
};
main();
```
</CodeGroup>


### Add a product-related metric
Can be done on Literal AI. 
Can be done by pushing on Posthog and others

### Add an AI evaluation result
Refer to [Evaluation](/guides/evaluation.mdx)

## Filter logs

TODO: Gif about filtering 

## Fetch existing logs
You can get all the threads from Literal AI to your program:
<CodeGroup>
```python Python
from literalai import LiteralClient

literalai_client = LiteralClient()

user_id = 'uuid'

threads = literalai_client.api.list_threads(
    first=5,
    filters=[{"operator": "eq", "field": "participantId", "value": user_id}]
)

for d in threads.data:
    print(d.to_dict())

literalai_client.flush_and_stop()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';
const literalAiClient = new LiteralClient("YOUR_API_KEY");

async function main() {

  const userId = 'uuid'
  const threads = await literalAiClient.api.getThreads({first: 5, filters: [{"operator": "eq", "field": "participantId", "value": userId}]});
  console.log(threads);
}
```
</CodeGroup>

More generally, you can fetch any Literal AI object. Check out the SDKs and API reference to learn how.

## Debug logged LLM generations

## Conclusion

Logging with Literal AI is composable and unopinionated. It can be done at different levels depending on your use case. 
