---
title: "Evaluation"
description: "Rules are used to automate actions on Literal AI based on specific conditions. Automated actions can range from evaluating an LLM generation to adding items in a dataset."
icon: ruler
---

## How to think about evaluation?

Scores are a crucial part of developing and improving your LLM application or agent.

There are multiple categories of Scores: 
1. **End-User Feedback - Explicit**: For example a thumbs-up or down to a chatbot's answer. This is automatically enabled when you have made a chatbot with [Chainlit](https://docs.chainlit.io/data-persistence/feedback). Otherwise, you can use the [score API](#programmatically) in your application.
2. **End-User Feedback - Implicit**: For example, user conversion to paid offering increases by 15% with new RAG version.
3. **AI Evaluation**: A score generated by an online evaluation done by an AI model. You can define metrics like faithfulness, hallucination, harmfulness etc.
4. **Human Evaluation**: A score given by a human developer, product owner or reviewer. You can define metrics like faithfulness, hallucination, harmfulness etc.

## Evaluate LLM logs in production automatically
Automating the evaluation of your [Run outputs or LLM generations](/concepts/logs#semantics) can really help to monitor and improve your LLM app in production, especially with large volumes of data.

### Configure an evaluation rule 

Go to the dashboard and click on `Configure Rules` to create/manage rules.

<Frame caption="Pick a Rule Type">
  <img src="/images/rule-type.png" alt="Pick a rule type" />
</Frame>

Once you picked a Rule Type, you will be able to update existing rules or create new ones.

<Frame caption="Manage your Rules">
  <img src="/images/rules.png" alt="Rules" />
</Frame>

A rule is composed of:

- **Name**: A name to identify the rule.
- **Sample Rate**: The percentage of outputs that will be evaluated by the rule.
- **Filters**: Additional conditions to decide if the rule should be triggered.
- **LLM**: The model to use for the evaluation.

#### Configure a score schema

A rule outputing a score will be based on a **Score Schema** which is a definition of the possible categories to evaluate the output.

A rule outputing a tag will be require a list of possible tags.

<Note>As of now, the prompt used for the evaluation is handled by Literal AI. We are working to allow you to specify your own eval prompt.</Note>

<Frame caption="Configure a Rule">
  <img src="/images/create-rule.png" alt="Configure Rule" />
</Frame>


### Evaluate with a custom eval metrics (sdk)

The SDKs provide score creation APIs with all fields exposed. Scores must be tied either
to a Step or a Generation object.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

score = literal_client.api.create_score(
            step_id="<STEP_UUID>",
            generation_id="<GENERATION_UUID>"
            name="user-feedback",
            type="HUMAN",
            comment="Hello world",
            value=1,
        )
```

```typescript TypeScript
import { LiteralClient, ChatGeneration} from "@literalai/client";

const literalClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

async function main() {
    const score = await literalClient.api.createScore({
        stepId: '05b217b4-5890-4434-b1a2-1749d842b2db',
        name: 'user-feedback',
        type: 'HUMAN',
        comment: 'Example score',
        value: 1,
      });
      
};
main();
```
</CodeGroup>

The example below highlights how you can receive human feedback on a
FastAPI server by calling the `create_score` API in Python.

```python server.py
import os
from fastapi import FastAPI
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

app = FastAPI()

@app.post("/feedback/")
def receive_human_feedback():

    # Create a score on the `Step` the user gave feedback for.
    return literal_client.api.create_score(
            type="HUMAN",
            name="user-feedback",
            value=0.9,
            comment="This was good",
            step_id="309a6a10-345b-4fd3-bbba-1dd1b435656c",
        )

receive_human_feedback()
```

## Label LLM logs on Literal AI

The Literal AI application offers an easy way to manage scores: Score Schema. 

Via Score Schemas, admin users can control and expose to users the various types of evaluations
allowed from the application. Score Templates come in two flavors: *Categorical* and *Continuous*. 

*Categorical* templates let you create a set of categories, each tied to a numeric value.
*Continuous* templates offer a minimum and a maximum value which users can then select from to score.


From a Step or a Generation, admins can then score by selecting a template and filling
in the required form fields.

For simple adding of Scores, you can do it from the Logs tab. 
If you plan to annotate a batch of logs, leverage the [annotation queue](/guides/annotation-queue.mdx)


## Automate actions based on evaluation results

### Add to Datasets/Annotation Queues

ðŸš§ Work in progress, coming soon ðŸš§

### Add labels

## Monitor evaluation results

Once your rules are set up, you can monitor their activity in the dashboard.

<Frame caption="Monitor your Rules">
  <img src="/images/monitor-rule.png" alt="Monitor Rules" />
</Frame>

You can see the number of invocation per rule as well as the average score of the evaluations.

You can also access the logs of the evaluations to look for potential errors.