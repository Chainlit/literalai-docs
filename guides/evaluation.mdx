---
title: "Evaluation"
icon: ruler
---

## Why evaluate?

Evaluation is key to enable continuous deployment of LLM-based applications and guarantee that newer
versions perform better than previous ones. To best capture the **user experience** one must understand the
multiple steps which make up the application. As AI applications grow in complexity, they tend to chain 
multiple steps.

Literal AI lets you log & monitor the various steps of your LLM application. By doing so, you can continuously improve the performance of your LLM system, building the most relevant metrics:

| Level          | Metrics                                       |
|----------------|--------------------------------------------|
| LLM Generation | Hallucination, Toxicity, etc.                 |
| Agent Run      | Task completion, Number of intermediate steps |
| Conversation Thread         | User satisfaction                             |


An example is the vanilla **Retrieval Augmented Generation** (RAG), which augments Large Language Models
(LLMs) with domain-specific data. Examples of metrics you can score against are: context relevancy, faithfulness, answer relevancy, etc.


## How to think about evaluation?

Scores are a crucial part of developing and improving your LLM application or agent.

| Who?  | When?           | Type of eval metrics | Example                                             |
|---------------|------------------|------------------|-----------------------------------------------------|
| End-User      | In Production     | Explicit Feedback (üëçüëé)        | Thumbs-up or down on a chatbot's answer             |
| End-User      | In Production     | Implicit Feedback based on product metric        | User conversion to paid offering increases by 15%   |
| LLM-as-a-Judge | In Production  | AI evaluation (without ground truth) | Hallucination, context relevancy, etc. |
| LLM-as-a-Judge | During Iteration  | AI evaluation against a Dataset (with ground truth or not) | Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc. |
| Domain Expert | During Iteration  | Human evaluation against a Dataset (with ground truth or not)  | Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc. |


## Means to score

### Score Schemas

Literal AI offers an easy way to manage scores: Score Schemas. 

With Score Schemas, admin users can control and expose to all users the various types of evaluations
allowed in their application. Score Schemas come in two flavors: 
- **Categorical** 
- **Continuous**. 

**Categorical** schemas let you create a set of categories, each tied to a numeric value.
**Continuous** schemas offer a minimum and a maximum value which users can then select from to score.

Categories have a description which is used to guide the scorer (be it Human or AI) in selecting the most 
relevant category.

Here's how to create a Score Schema:

<Frame caption="Create Score Schema">
  <img
    src="/images/create-score-schema.gif"
    alt="Create Score Schema"
  />
</Frame>

Feel free to browse your steps from the Logs tab and simply add a Human Review directly on your `Step` or `Generation` entities:

<Frame caption="Score a Generation">
  <img
    src="/images/score-generation.gif"
    alt="Score a Generation"
  />
</Frame>

If you want to go through a wide range of logs, you can leverage the [annotation queues](/guides/annotation-queue) 
which makes it easier to streamline the review process.

### Create a `Scorer`

Scorers are the basic building blocks of evaluation. 
They are used to score `Steps` or `Generations` based on a `Score Schema`.

To create a scorer, go to the `Scorers` page and click on the `+` button in the upper right corner of the table.
Here's an example of the flow:

<Frame caption="Create Scorer">
  <img
    src="/images/create-scorer.gif"
    alt="Create Scorer"
  />
</Frame>

Once you have created a scorer, you can use it in multiple ways:
- either with Online Evals
- or by running a Prompt experiment from the Playground


### Evaluate LLM logs in production automatically

Automating the evaluation of your [Run outputs or LLM generations](/guides/logs#semantics) can significantly help
detect patterns and areas of improvement for your LLM app in production, especially with large volumes of data.

To manage your Online Evals, go to the `Online Evals` page.
You can also access that page from the Dashboard by clicking on `Configure Online Evals`.

You will be able to update existing rules or create new ones:

<Frame caption="Manage your Rules">
  <img src="/images/rules.png" alt="Rules" />
</Frame>

An Online Eval is composed of:

- **Name**: A name to identify the rule.
- **Log Type**: Either `Agent Run` or `LLM Generation`, it's the target to evaluate.
- **Sample Rate**: The percentage of logs to evaluate.
- **Filters**: Additional conditions to selectively evaluate certain logs.
- **Scorer**: The scorer to use for the evaluation.

To create an Online Eval, go to the `Online Evals` page and click on the `+` button in the upper right corner of the table.

<Frame caption="Create Online Eval">
  <img src="/images/create-online-eval.gif" alt="Create Online Eval" />
</Frame>

### Evaluate with a custom eval metrics (SDK)

The SDKs provide score creation APIs with all fields exposed. Scores must be tied either
to a Step or a Generation object.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

score = literalai_client.api.create_score(
            step_id="<STEP_UUID>",
            name="context-relevancy",
            type="AI",
            comment="context is relevant to the question",
            value=0.9,
        )
```

```typescript TypeScript
import { LiteralClient, ChatGeneration} from "@literalai/client";

const literalAiClient = new LiteralClient(process.env["LITERAL_API_KEY"]);

async function main() {
    const score = await literalAiClient.api.createScore({
        stepId: '<STEP_UUID>',
        name: 'context-relevancy',
        type: 'AI',
        comment: 'context is relevant to the question',
        value: 0.9,
      });
      
};
main();
```
</CodeGroup>

## Automate actions based on evaluation results

### Add to Datasets/Annotation Queues

üöß Work in progress, coming soon üöß

### Add labels

## Monitor evaluation results

Once your rules are set up, you can monitor their activity in the dashboard.

<Frame caption="Monitor your Rules">
  <img src="/images/monitor-rule.png" alt="Monitor Rules" />
</Frame>

You can see the number of invocation per rule as well as the average score of the evaluations.

You can also access the logs of the evaluations to look for potential errors.