---
title: "Llama Index"
icon: check
---

The Llama Index integration enables to monitor your RAG pipelines with a single line of code.

<Tip>The Llama Index integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](/integrations/openai).</Tip>

<Warning>
  The Llama Index integration in the Python SDK is compatible with Llama Index starting at version 0.10.58.
</Warning>

<CodeGroup>
```python Python
from literalai import LiteralClient
from llama_index.core import Document, VectorStoreIndex

client = LiteralClient()
# The magic happens here
client.instrument_llamaindex()

index = VectorStoreIndex.from_documents([Document.example()])
query_engine = index.as_query_engine()

# Here we are outside of a thread so the integration will create one for us
query_engine.query("Tell me about LLMs")

# If we call Llamaindex from inside a context, the integration will use it
with client.thread(name="Test Llamaindex Thread") as thread:
    query_engine.query("Tell me about LLMs")

# If we make several calls inside the context, they will all be logged in the same thread
with client.thread(name="Llamaindex Questions") as thread:
    query_engine.query("Tell me about LLMs")
    query_engine.query("What is RAG ?")

# Optional: make sure to flush the events before stopping the client
client.flush_and_stop()

```
</CodeGroup>

Each Thread will result in the following tree on Literal AI :

<Frame caption="A Llamaindex RAG thread on Literal AI">
  <img src="/images/llamaindex-thread.png" alt="A Llamaindex RAG thread on Literal AI" />
</Frame>
