---
title: "LangChain/LangGraph"
---

The Langchain integration enables to monitor your Langchain agents and chains with a single line of code.

<Note>You should create a new instance of the callback handler for each invocation.</Note>

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

from langchain_openai import ChatOpenAI
from langchain.schema.runnable.config import RunnableConfig
from langchain.schema import StrOutputParser
from langchain.prompts import ChatPromptTemplate

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

cb = literal_client.langchain_callback()

prompt = ChatPromptTemplate.from_messages(
    ['human', 'Tell me a short joke about {topic}']
)

model = ChatOpenAI(streaming=True)
runnable = prompt | model | StrOutputParser()

res = runnable.invoke(
    {"topic": "ice cream"},
    config=RunnableConfig(callbacks=[cb], run_name="joke")
    )
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { ChatOpenAI } from '@langchain/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']); // This is the default and can be omitted

const cb = literalClient.instrumentation.langchain.literalCallback();

// Example of using the callback
const prompt = ChatPromptTemplate.fromMessages([
    ['human', 'Tell me a short joke about {topic}']
  ]);

const model = new ChatOpenAI({});
const outputParser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(outputParser);

async function main() {
    const response = await chain.invoke(
    {
        topic: 'ice cream'
    },
    {
        runName: 'joke',
        callbacks: [cb]
    }
    );
}
main();
```
</CodeGroup>

## Multiple langchain calls in a single thread

You can combine the Langchain callback handler with the concept of [Thread](/concepts/logs) to monitor multiple langchain calls in a single thread.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

with literal_client.thread(name="Langchain example") as thread:
    cb = literal_client.langchain_callback()
    # Call your Langchain agent here
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']); // This is the default and can be omitted

const thread = await literalClient.thread({ name: "Langchain Example" }).upsert();
const cb = literalClient.instrumentation.langchain.literalCallback(thread.id);

// Call your Langchain agent here
```
</CodeGroup>


## LangGraph

LangGraph works similarly to LangChain when it comes to using the Literal AI callback handler.

<CodeGroup>
```python Python
# list imports 

workflow = StateGraph(MessagesState)
# define graph ...

app = workflow.compile(checkpointer=checkpointer)

# run the app with LangChain callback handler
cb = literal_client.langchain_callback()
final_state = app.invoke(
    {"messages": [HumanMessage(content="what is the weather in sf")]},
    config=RunnableConfig(callbacks=[cb])
)
```
</CodeGroup>

<Card title="Full LangGraph Example" icon="play" href="/tutorials/langgraph-example">
  Check out this LangGraph example.
</Card>
