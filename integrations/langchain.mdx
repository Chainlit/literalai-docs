---
title: "LangChain/LangGraph"
icon: check
---

The Langchain integration enables to monitor your Langchain agents and chains with a single line of code.

<Note>You should create a new instance of the callback handler for each invocation.</Note>

<Tip>The LangChain integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](/integrations/openai).</Tip>

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

from langchain_openai import ChatOpenAI
from langchain.schema.runnable.config import RunnableConfig
from langchain.schema import StrOutputParser
from langchain.prompts import ChatPromptTemplate

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

cb = literal_client.langchain_callback()

prompt = ChatPromptTemplate.from_messages(
    ['human', 'Tell me a short joke about {topic}']
)

model = ChatOpenAI(streaming=True)
runnable = prompt | model | StrOutputParser()

res = runnable.invoke(
    {"topic": "ice cream"},
    config=RunnableConfig(callbacks=[cb], run_name="joke")
    )
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { ChatOpenAI } from '@langchain/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']); // This is the default and can be omitted

const cb = literalClient.instrumentation.langchain.literalCallback({
  // You can add a list of chain types you want to ignore on Literal AI
  chainTypesToIgnore: ["ChatPromptTemplate"],
});

const prompt = ChatPromptTemplate.fromMessages([
    ['human', 'Tell me a short joke about {topic}']
  ]);

const model = new ChatOpenAI({});
const outputParser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(outputParser);

async function main() {
  const response = await chain.invoke(
    { topic: "ice cream" },
    {
      // If you give your run a LangChain run name, it will also be named like that in Literal AI
      runName: "That one Ice Cream joke",
      callbacks: [cb],
    }
  );
}
main();
```
</CodeGroup>

## Multiple langchain calls in a single thread

You can combine the Langchain callback handler with the concept of [Thread](/guides/logs) to monitor multiple langchain calls in a single thread.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

with literal_client.thread(name="Langchain example") as thread:
    cb = literal_client.langchain_callback()
    # Call your Langchain agent here
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const literalClient = new LiteralClient();

// You have three options to group multiple langchain calls in a single thread

/**
 * The first option is to add a threadId to the callback. Every time you run this code, a new
 * thread called "Jokes" will be created
*/
const cb = literalClient.instrumentation.langchain.literalCallback({
  threadId: "Jokes",
});

async function main() {
  const response = await chain.invoke(
    { topic: "ice cream" },
    { callbacks: [cb] }
  );
}
main();

/**
 * The second option is to use LangChain's built-in thread_id concept.
 * This will create a persistent thread, meaning every time you run this code, the new runs
 * and generations will be stacked inside the existing thread
 */
const cb = literalClient.instrumentation.langchain.literalCallback();

async function main() {
  const response = await chain.invoke(
    { topic: "ice cream" },
    {
      callbacks: [cb],
      configurable: { thread_id: "Jokes" },
    }
  );
}

/**
 * Alternatively, you can still use the Literal AI thread or step wrapper to group multiple langchain calls
 * Like in the first option, this will create a new thread every time you run this code
*/

async function main() {
  await literalClient.thread({ name: "Jokes" }).wrap(async () => {
    const response = await chain.invoke(
      { topic: "ice cream" },
      {
        callbacks: [cb],
        configurable: { thread_id: "Jokes" },
      }
    );
  });
}

```
</CodeGroup>


## LangGraph

LangGraph works similarly to LangChain when it comes to using the Literal AI callback handler.

<CodeGroup>
```python Python
# list imports 

workflow = StateGraph(MessagesState)
# define graph ...

app = workflow.compile(checkpointer=checkpointer)

# run the app with LangChain callback handler
cb = literal_client.langchain_callback()
final_state = app.invoke(
    {"messages": [HumanMessage(content="what is the weather in sf")]},
    config=RunnableConfig(callbacks=[cb])
)
```

```typescript Typescript
const cb = literalClient.instrumentation.langchain.literalCallback();

// Define graph, nodes and edges
const graphState: StateGraphArgs<AgentState>["channels"] = {
  // ...
};

const workflow = new StateGraph<AgentState>({ channels: graphState })
  .addNode(...);

const app = workflow.compile();

// Invoke the graph with the callback handler
const finalState = await app.invoke(
  { messages: [new HumanMessage("what is the weather in sf")] },
  {
    configurable: { thread_id: "Weather Thread" },
    runName: "weather",
    callbacks: [cb],
  }
);
```
</CodeGroup>

<CardGroup cols={2}>
<Card title="Python" icon="python" href="/tutorials/langgraph-example-py">
  Check out this Python LangGraph example.
</Card>
<Card title="Typescript" icon="node" href="/tutorials/langgraph-example-ts">
  Check out this Typescript LangGraph example.
</Card>
</CardGroup>