---
title: "Vercel AI SDK"
icon: check
---

This integration allows you to very simply add observability and monitoring to your LLM application based on [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction). The instrumentation is available for the two main methods of the Vercel AI SDK: `generateText` and `streamText`.

<Tip>The Vercel AI SDK integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](/integrations/openai).</Tip>

<CodeGroup>
```typescript generateText
import { LiteralClient } from '@literalai/client';
import { generateText as baseGenerateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']);

// Here we wrap `generateText` with the Literal AI instrumentation. This will allow us to automatically capture
// both the response from the LLM API and the other informations present in the response (latency, token counts, etc...).
const generateText = literalClient.instrumentation.vercel.instrument(baseGenerateText);

export async function POST(req: Request) {
  // The wrapped version of `generateText` accepts the same parameters as the original function.
  const { text } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: question
  });

  return { text };
}
```

```typescript streamText
import { LiteralClient } from '@literalai/client';
import { streamText as baseStreamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Here we wrap `streamText` with the Literal AI instrumentation. This will allow us to automatically capture
// both the response from the LLM API and the other informations present in the response (latency, token counts, etc...).
const streamText = literalClient.instrumentation.vercel.instrument(baseStreamText);

export async function POST(req: Request) {
  // The wrapped version of `streamText` accepts the same parameters as the original function.
  const { textStream } = await streamText({
    model: openai('gpt-3.5-turbo'),
    system: question,
    messages: history
  });

  return textStream;
}
```
</CodeGroup>


## With Threads and Runs

In most cases, you will want to keep track of the different generations from your application by grouping them into [Threads or Runs](/concepts/logs). This is especially useful when you want to understand the context in which a generation was made, or when you want to compare different generations.

```typescript TypeScript
export async function POST(req: Request) {
  await literalClient.thread({ name: 'Example' }).wrap(async () => {
    const { text } = await generateText({
      model: openai('gpt-3.5-turbo'),
      prompt: question,
    });

    return { text };
  });
}
```

## Cookbooks

You can find more involved examples in our Cookbooks repository :

* [This chatbot](https://github.com/Chainlit/literal-cookbook/blob/main/typescript/nextjs-openai) uses Vercel AI SDK's `useChat` hook in the frontend
* [This example](https://github.com/Chainlit/literal-cookbook/blob/main/typescript/vercel-ai-sdk) uses the Vercel AI SDK integration in the backend
