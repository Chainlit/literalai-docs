---
title: "Vercel AI SDK"
---


This integration allows you to very simply add observability and monitoring to your LLM application based on [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction). The instrumentation is available for the two main methods of the Vercel AI SDK, `generateText` and `streamText`.

<CodeGroup>
```typescript generateText
import { LiteralClient } from '@literalai/client';
import { generateText as baseGenerateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']);

// Here we wrap `generateText` with the Literal AI instrumentation. This will allow us to automatically capture
// both the response from the LLM API and the other informations present in the response (latency, token counts, etc...).
const generateText = literalClient.instrumentation.vercel.instrument(baseGenerateText);

export async function POST(req: Request) {
  // The wrapped version of `generateText` accepts the same parameters as the original function.
  const { text } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: question
  });

  return { text };
}
```

```typescript streamText
import { LiteralClient } from '@literalai/client';
import { streamText as baseStreamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Here we wrap `streamText` with the Literal AI instrumentation. This will allow us to automatically capture
// both the response from the LLM API and the other informations present in the response (latency, token counts, etc...).
const streamText = literalClient.instrumentation.vercel.instrument(baseStreamText);

export async function POST(req: Request) {
  const { textStream } = await streamText({
    model: openai('gpt-3.5-turbo'),
    system: question,
    messages: history
  });

  return textStream;
}
```
</CodeGroup>


## With Threads and Steps

You can pass a [Threads](/concepts/observability/thread) or a [Steps](/concepts/observability/step) as an option of your generations to keep your logs organized.

<CodeGroup>
```typescript TypeScript
import { LiteralClient } from '@literalai/client';

import { generateText as baseGenerateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']); // This is the default and can be omitted

const generateText = literalClient.instrumentation.vercel.instrument(baseGenerateText);

export async function getAnswer(question: string) {
  const thread = await literalClient.thread({ name: "Example" }).upsert();

  const { text } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: question,
    literalAiParent: thread,
  });

  return { text };
}
```
</CodeGroup>
