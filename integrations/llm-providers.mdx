---
title: "Other LLM Providers (Anthropic, etc.)"
icon: check
---

Monitor all LLM providers via LangChain simple unified interface `init_chat_model`:

<CodeGroup>
```py Python
from langchain.chat_models import init_chat_model
from literalai import LiteralClient

literalai_client = LiteralClient()
gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
claude_sonnet = init_chat_model("claude-3-5-sonnet-20241022", model_provider="anthropic", temperature=0)

# Literal AI callback
cb = literalai_client.langchain_callback()

# Invoke the model with input and callback configuration
gpt_4o.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))
claude_sonnet.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))

literalai_client.flush()
```


```typescript TypeScript
import { LiteralClient } from "@literalai/client";

import { HumanMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";

const literalai_client = new LiteralClient(process.env.LITERAL_API_KEY);
const cb = literalai_client.instrumentation.langchain.literalCallback();

// Example of using the callback
const message = new HumanMessage({ content: "What is the capital of France?" });

const model = new ChatAnthropic({
  anthropicApiKey: process.env.ANTHROPIC_API_KEY,
});
model.invoke([message], {
  callbacks: [cb],
}).then(response => {
  console.log(response)
}).catch(error => {
  // Handle any errors here
});
```
</CodeGroup>

You can thus monitor Anthropic, Mistral AI, Cohere and many other LLM providers. Full list [here](https://python.langchain.com/docs/integrations/platforms/).
